import numpy as np
import pandas as pd
import re
import json
import os
from konlpy.tag import Okt
from tensorflow.python.keras.preprocessing.sequence import pad_sequences
from tensorflow.python.keras.preprocessing.text import Tokenizer

dirnm = './tensorflow/'
train_data = pd.read_csv(dirnm+'ratings_train.txt',header=0, delimiter='\t',quoting=3)

def preprocessing(review, okt, remove_stopwords = False, stop_words =[]):
    review_text = re.sub("[^가-힣ㄱ-하-ㅣ\\s]","",review)
    word_review = okt.morphs(review_text, stem=True)
    
    if remove_stopwords:
        word_review = [token for token in word_review if not token in stop_words]
    return word_review

okt=Okt()

# 불용어 사전 생성
stop_words = set(['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한'])

clean_train_review = []

for review in train_data['document']:
    if type(review) == str:
        clean_train_review.append(preprocessing(review, okt, remove_stopwords=True, stop_words=stop_words))
    else:
        clean_train_review.append([])
        
test_data = pd.read_csv(dirnm+'ratings_test.txt', header=0, delimiter='\t',quoting=3)
clean_test_review = []

for review_test in test_data['document']:
    if type(review_test) == str:
        clean_test_review.append(preprocessing(review_test, okt, remove_stopwords=True,
                                              stop_words=stop_words))
    else:
        clean_test_review.append([])

tokenizer = Tokenizer()
tokenizer.fit_on_texts(clean_train_review)
train_sequences = tokenizer.texts_to_sequences(clean_test_review)
test_sequences = tokenizer.texts_to_sequences(clean_test_review)

# 단어 사전 형태
word_vocab = tokenizer.word_index
MAX_SEQUENCE_LENGTH=8 #문장 최대 길이

#학습 데이터를 벡터화
train_inputs = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH,
                            padding='post')
train_labels = np.array(train_data['label'])# 학습데이터의 라벨

test_inputs = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH,
                           padding='post')#평가 데이터를 벡터화
test_labels = np.array(test_data['label']) #평가 데이터의 라벨

# 데이터 저장
train_input_data = 'train_input.npy'
train_label_data = 'train_label.npy'
test_input_data = 'test_input.npy'
test_label_data = 'test_label.npy'
data_configs = 'data_configs.json'

data_configs = {}

data_configs['vocab'] = word_vocab
data_configs['vocab_size'] = len(word_vocab)+1 # vocab size 추가

if not os.path.exists(dirnm): #저장하는 디렉토리가 존재하지 않으면 생성
    os.makedirs(dirnm)

np.save(open(dirnm+train_input_data,'wb'),train_inputs)
np.save(open(dirnm+train_label_data,'wb'),train_labels)
np.save(open(dirnm+test_input_data,'wb'),test_inputs)
np.save(open(dirnm+test_label_data,'wb'),test_labels)

# 데이터 사전을 json형태로 저장

json.dump(data_configs, open(dirnm+data_configs,'w'), ensure_ascii=False)
