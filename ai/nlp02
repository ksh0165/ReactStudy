# 감정 분석 , 학습데이터 불러오기
import os
from datetime import datetime
import tensorflow as tf
import numpy as np
import json
from sklearn.model_selection import train_test_split

#저장된 데이터 파일 불러오기
dirnm = './tensorflow/'
input_train_data = 'train_input.npy'
label_train_data = 'train_label.npy'

input_data = np.load(open(dirnm+input_train_data,'rb'))
label_data = np.load(open(dirnm+label_train_data,'rb'))

# 데이터를 모델에 적용 전 학습데이터의 일부를 검증 데이터로 분리해 성능 측정
# 학습 90% 훈련 10%
TEST_SPLIT=0.1
RNG_SEED = 13371447

input_train, input_eval, label_train, label_eval = train_test_split(input_data,label_data, test_size=TEST_SPLIT, random_state=RNG_SEED)

-------------------------------------------------------------------------

# 감정 분석 , 학습데이터 불러오기
import os
from datetime import datetime
import tensorflow as tf
import numpy as np
import json
from sklearn.model_selection import train_test_split

# 학습함수화 검증 함수
def mapping_fn(X,Y):
    input, label = {'x':X}, Y
    return input, label

def train_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train))
    dataset = dataset.shuffle(buffer_size=len(input_train))
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.map(mapping_fn)
    dataset = dataset.repeat(count=NUM_EPOCHS)
    oterator = dataset.make_one_shot_iterator()
    
    return iterator.get_next()

def eval_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((input_eval,label_eval))
    dataset = dataset.shuffle(buffer_size=len(input_eval))
    dataset = dataset.batch(16)
    dataset = dataset.map(mapping_fn)
    iterator = dataset.make_one_shot_iterator()
    
    return iterator.get_next()


dirnm = './tensorflow/'
input_train_data = 'train_input.npy'
label_train_data = 'train_label.npy'

input_data = np.load(open(dirnm+input_train_data,'rb'))
label_data = np.load(open(dirnm+label_train_data,'rb'))

# 데이터를 모델에 적용 전 학습데이터의 일부를 검증 데이터로 분리해 성능 측정
# 학습 90% 훈련 10%
TEST_SPLIT=0.1
RNG_SEED = 13371447

input_train, input_eval, label_train, label_eval = train_test_split(input_data,label_data, test_size=TEST_SPLIT, random_state=RNG_SEED)

# 모델 함수 구현

------------------------------------------------------------------
# 감정 분석 , 학습데이터 불러오기
import os
from datetime import datetime
import tensorflow as tf
import numpy as np
import json
from sklearn.model_selection import train_test_split

# 학습함수화 검증 함수
def mapping_fn(X,Y):
    input, label = {'x':X}, Y
    return input, label

def train_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train))
    dataset = dataset.shuffle(buffer_size=len(input_train))
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.map(mapping_fn)
    dataset = dataset.repeat(count=NUM_EPOCHS)
    oterator = dataset.make_one_shot_iterator()
    
    return iterator.get_next()

def eval_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((input_eval,label_eval))
    dataset = dataset.shuffle(buffer_size=len(input_eval))
    dataset = dataset.batch(16)
    dataset = dataset.map(mapping_fn)
    iterator = dataset.make_one_shot_iterator()
    
    return iterator.get_next()

# 에스티메이터에 데이터를 적용하기 위해 데이터 입력 함수 정의
# 모델 함수 구현
def model_fn(features, labels, mode, params):
    TRAIN = mode == tf.estimator.ModeKeys.TRAIN
    EVAL = mode == tf.estimator.ModeKeys.EVAL
    PREDICT = mode == tf.setimator.ModeKeys.PREDICT
    
    embedding_layer = tf.keras.layers.Embedding(
    VOCAB_SIZE,
    EMB_SIZE)(features['x'])
    
    dropout_emb = tf.keras.layers.Dropout(rate=0.2)(embedding_layer)
    
    conv = tf.keras.layers.Conv1d(
    filters=32,
    kernel_size=3,
    padding='same',
    activation=tf.nn.relu)(dropout_emb)
    
    pool = tf.keras.layers.GlobalMaxPool1D()(conv)
    
    hidden = tf.keras.layers.Dense(units=250, activation=tf.nn.relu)(pool)

    dropout_hidden = tf.keras.layers.Dropout(rate=0.2, training=TRAIN)(hidden)
    logits = tf.keras.layers.Dense(units=1)(dropout_hidden)
    
    if labels is not None:
        labels = tf.reshape(labels, [-1,1])
        
    if TRAIN:
        global_step = tf.train.get_global_step()
        loss = tf.losses.sigmoid_cross_entropy(labels, logits)
        train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)
        
        return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss= loss)
    
    elif EVAL:
        loss = tf.losses.sigmoid_cross_entropy(labels, logits)
        pred = tf.nn.sigmoid(logits)
        accuracy = tf.metrics.accuraty(labels, tf.round(pred))
        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops={'acc':accurary})
    
    elif PREDICT:
        return tf.estimator.EstimatorSpec(
        mode=mode,
            predictions={
                'prob': tf.nn.sigmoid(logits),
            }
        )

BATCH_SIZE = 16
NUM_EPOCHS = 10
#vocab_size = prepro_configs['vocab_size']
embedding_size = 128

# 에스티메이터 객체를 정의한 후, 학습
est = tf.estimator.Estimator(model_fn, model_dir="./tensorflow/cnn_model")

#train 함수를 호출해 학습한다
time_start = datetime.utcnow()
print("Experiment started at {}".format(time_start.strftime("%H:%M:%S")))
print("....................................")

est.train(train_input_fn)

time_end = datetime.utcnow()
print("....................................")
print("Experiment finished at {}".format(time_end.strftime("%H:%M:%S")))

print("")
time_elapsed = time_end - time_start
print("Experiment elapsed time: {} seconds".format(time_elapsed.total_seconds()))

dirnm = './tensorflow/'
input_train_data = 'train_input.npy'
label_train_data = 'train_label.npy'

input_data = np.load(open(dirnm+input_train_data,'rb'))
label_data = np.load(open(dirnm+label_train_data,'rb'))

# 데이터를 모델에 적용 전 학습데이터의 일부를 검증 데이터로 분리해 성능 측정
# 학습 90% 훈련 10%
TEST_SPLIT=0.1
RNG_SEED = 13371447

input_train, input_eval, label_train, label_eval = train_test_split(input_data,label_data, test_size=TEST_SPLIT, random_state=RNG_SEED)

