import sklearn
from sklearn.datasets import load_iris

iris_dataset = load_iris()
print("iris_dataset key: {}".format(iris_dataset.keys()))

------------------------------------------------------------
import sklearn
from sklearn.model_selection import train_test_split

train_input, test_input, train_label, test_label = train_test_split(iris_dataset['data'],
                                                                   iris_dataset['target'], test_size=0.25, random_state=42)
print("shape of train_input: {}".format(train_input.shape))
print("shape of test_input: {}".format(test_input.shape))
print("shape of train_label: {}".format(train_label.shape))
print("shape of test_label: {}".format(test_label.shape))

-----------------------------------------------------------
pip install -U scikit-learn

# anacondat prompt에서 설치 진행


from sklearn.feature_extraction.text import CountVectorizer

text_data = ['나는 배가 고프다','내일 점심 뭐먹지',
            '내일 공부 해야겠다']

count_vectorizer = CountVectorizer()

count_vectorizer.fit(text_data)
print(count_vectorizer.vocabulary_)

sentence = [text_data[0]]
print(count_vectorizer.transform(sentence).toarray())

----------------------------------------------------------

from sklearn.feature_extraction.text import TfidfVectorizer

text_data = ['나는 배가 고프다','내일 점심 뭐먹지',
            '내일 공부 해야겠다']

tfidf_vectorizer = TfidfVectorizer()

tfidf_vectorizer.fit(text_data)
print(tfidf_vectorizer.vocabulary_)

sentence = [text_data[2]]
print(tfidf_vectorizer.transform(sentence).toarray())

---------------------------------------------------------------

# JPype1 홈페이지 https://konlpy.org/ko/v0.5.1/install/#id2   pip3 install konlpy
# https://www.lfd.uci.edu/~gohlke/pythonlibs/#jpype -> whl 파일 다운로드 및 설치
3 python -m pip install c:\JPype1-0.7.3-cp38-cp38-win_amd64.whl
import konlpy
from konlpy.tag import Okt

okt = Okt()

text = "한글 자연어 처리는 재밋다 이제부터 열심히 해야지 ㅎㅎㅎ"
print(okt.morphs(text))
print(okt.morphs(text, stem=True))

print(okt.nouns(text))
print(okt.phrases(text))

print(okt.pos(text))
print(okt.pos(text, join=True))
---------------------------------------

https://github.com/e9t/nsmc

------------------------------------------

# 데이터 분석 단계

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

dirs = './tensorflow/'
train_data = pd.read_csv(dirs+'ratings_train.txt' , header=0, delimiter='\t', quoting=3)
train_data.head()

print('전체 학습 데이터의 개수: {}'.format(len(train_data)))

# 도규먼트 열 데이터 확인
train_lenght = train_data['document'].astype(str).apply(len)
train_lenght.head()
#그래프
plt.figure(figsize=(12,5))
plt.hist(train_lenght, bins=200, alpha=0.5, color='r', label='word')
plt.yscale('log', nonposy='clip')

plt.title('Log-Histogram of length of review')
plt.xlabel('Length of review')

plt.ylabel('Number of review')

print('리뷰 길이 최댓값: {}'.format(np.max(train_lenght)))
print('리뷰 길이 최소값: {}'.format(np.min(train_lenght)))
print('리뷰 길이 평균값: {}'.format(np.mean(train_lenght)))
print('리뷰 길이 표준편차: {}'.format(np.std(train_lenght)))
print('리뷰 길이 중간값: {}'.format(np.median(train_lenght)))
print('리뷰 길이 제1사분위: {}'.format(np.percentile(train_lenght,25)))
print('리뷰 길이 제3사분위: {}'.format(np.percentile(train_lenght,75)))

# 박스 그래프
plt.figure(figsize=(12,5))

plt.boxplot(train_lenght,
           labels=['counts'],
           showmeans=True)

# 긍정 부정 데이터 비율
fig, axe = plt.subplots(ncols=1)
fig.set_size_inches(6,3)
sns.countplot(train_data['label'])


print("긍정 리뷰 개수: {}".format(train_data['label'].value_counts()[1]))
print("부정 리뷰 개수: {}".format(train_data['label'].value_counts()[0]))

#리뷰 단어 길이 그래프
train_word_counts = train_data['document'].astype(str).apply(lambda x:len(x.split(' ')))

plt.figure(figsize=(15,10))
plt.hist(train_word_counts, bins=50, facecolor='r', label='train')
plt.title('Log-Histogram of word count in review', fontsize=15)
plt.yscale('log',nonposy='clip')
plt.legend()
plt.xlabel('Number of words', fontsize=15)
plt.ylabel('Number of reviews', fontsize=15)

print('리뷰 단어 개수 최댓값: {}'.format(np.max(train_word_counts)))
print('리뷰 단어 개수 최소값: {}'.format(np.min(train_word_counts)))
print('리뷰 단어 개수 평균값: {}'.format(np.mean(train_word_counts)))
print('리뷰 단어 개수 표준편차: {}'.format(np.std(train_word_counts)))
print('리뷰 단어 개수 중간값: {}'.format(np.median(train_word_counts)))
print('리뷰 단어 개수 제1사분위: {}'.format(np.percentile(train_word_counts,25)))
print('리뷰 단어 개수 제3사분위: {}'.format(np.percentile(train_word_counts,75)))

# 특수문자 있는지 확인
qmarks = np.mean(train_data['document'].astype(str).apply(lambda x: '?' in x))

fullstop = np.mean(train_data['document'].astype(str).apply(lambda x: '.' in x))

print('물음표가 있는 질문: {:.2f}%'.format(qmarks * 100))
print('마침표가 있는 질문: {:.2f}%'.format(fullstop * 100))
------------------------------------------------------
# 데이터 전처리
